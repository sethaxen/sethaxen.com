<?xml version="1.0" encoding="UTF-8"?>

<rss version="2.0"
  xmlns:content="http://purl.org/rss/1.0/modules/content/"
  xmlns:dc="http://purl.org/dc/elements/1.1/"
  xmlns:media="http://search.yahoo.com/mrss/"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:georss="http://www.georss.org/georss">

  <channel>
    <title>
      <![CDATA[  Seth Axen&#39;s blog  ]]>
    </title>
    <link> https://sethaxen.com/blog </link>
    <description>
      <![CDATA[  Random thoughts on math, statistics, and code that I haven&#39;t felt like turning into papers.  ]]>
    </description>
    <atom:link
      href="https://sethaxen.com/blog/feed.xml"
      rel="self"
      type="application/rss+xml" />


<item>
  <title>
    <![CDATA[  Differentiating the LU decomposition  ]]>
  </title>
  <link> https://sethaxen.com/blog/blog/2021/02/differentiating-the-lu-decomposition/index.html </link>
  <guid> https://sethaxen.com/blog/blog/2021/02/differentiating-the-lu-decomposition/index.html </guid>
  <description>
    <![CDATA[  Deriving differentiation rules for the LU decomposition of square and non-square matrices.  ]]>
  </description>  
  
  <content:encoded>
    <![CDATA[  
<p><em>I was implementing differentiation rules of the LU decomposition in <a href="https://github.com/JuliaDiff/ChainRules.jl">ChainRules.jl</a> and needed rules that supported non-square matrices, so I worked them out.</em></p>
<div class="franklin-toc"><ol><li>Introduction</li><li>Square \(A\)</li><li>Wide \(A\)</li><li>Tall \(A\)</li><li>Implementation</li><li>Conclusion</li><li>References</li></ol></div>
<h2 id="introduction">Introduction</h2>
<p>For a square matrix \(A \in \mathbb{C}^{m \times n}\), the <a href="https://en.wikipedia.org/wiki/LU_decomposition">LU decomposition</a> is useful for solving systems of equations, inverting \(A\), and computing the determinant.</p>
<p>This post assumes familiarity with terminology of forward- and reverse-mode differentiation rules. For a succinct review, I recommend reading <a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/arrays.html">the ChainRules guide on deriving array rules</a>.</p>
<h3 id="definition">Definition</h3>
<p>The LU decomposition of \(A\) is</p>
\[P A = L U.\]
<p>Where \(q = \min(m, n)\), \(L \in \mathbb{C}^{m \times q}\) is a unit lower triangular matrix, that is, a lower triangular matrix whose diagonal entries are all ones. \(U \in \mathbb{C}^{q \times n}\) is an upper triangular matrix. \(P \in \mathbb{R}^{m \times m}\) is a <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrix</a> whose action on a matrix \(X\) &#40;i.e. \(P X\)&#41; reorders the rows of \(X\). As a permutation matrix, \(P P^\mathrm{T} = P^\mathrm{T} P = I\).</p>
<h3 id="uses">Uses</h3>
<p>If we have a system of the form of \(A X = B\) and would like to solve for \(X\), we can use the LU decomposition to write \(L U X = P B\). Then \(X = L^{-1} (U^{-1} (P B))\). Note that the row-swapping action \(P B\) can be computed in-place. We can also easily compute the left-division by the triangular matrices in-place using <a href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution">forward and back substitution</a>.</p>
<p>By setting \(B\) in the above equation to be the identity matrix, then we can compute the inverse of \(A\), that is \(X = A^{-1}\).</p>
<p>The determinant of \(A\) is \(\det(A) = \det(P) \det(L) \det(U)\). The determinant of a triangular matrix is the product of its diagonal entries, so \(\det(L) = 1\), and \(\det(U) = \prod_{i=1}^n U_{ii}\). \(\det(P) = (-1)^s\), where \(s\) is the number of row swaps encoded by the permutation matrix. So \(\det(A) = (-1)^s \prod_{i=1}^n U_{ii}\), which is very cheap to compute from the decomposition.</p>
<p>The LU decomposition can still be computed when \(A\) is wide &#40;\(m < n\)&#41; or tall &#40;\(m > n\)&#41;. However, none of the above applications make sense in this case, and I don&#39;t know what it&#39;s useful for.</p>
<h3 id="motivation">Motivation</h3>
<p>Often the LU decomposition is computed using LAPACK subroutines, which cannot be automatically differentiated through. Hence, it is necessary to implement custom automatic differentiation rules. <sup id="fnref:HoogAnderssenLukas2011">[1]</sup> derived a pushforward &#40;AKA forward-mode differentiation rule or Jacobian-vector-product&#41; for the LU decomposition for the square case, but I couldn&#39;t find a rule for the wide or tall cases. This is a problem, because I wanted to implement a generic rule for ChainRules.jl that would work for all dense matrices in Julia, where square and non-square dense matrices are all implemented using the same type <code>Matrix</code> &#40;more specifically, as the union of types <code>StridedMatrix</code>&#41;. It is not ideal to write a custom rule that will work for only a subset of matrices of a given type.</p>
<p>We could always pad \(A\) with zeros to make it square. However, if \(m \ll n\) or \(m \gg n\), then this is wasteful. JAX seems to <a href="https://github.com/google/jax/blob/jax-v0.2.9/jax/_src/lax/linalg.py#L826-L871">use this approach</a>, though it&#39;s possible that internally it does something fancier and doesn&#39;t explicitly allocate or operate on the padding.</p>
<p>Thankfully, it&#39;s not too hard to work out the pushforwards and pullbacks for the non-square case by splitting the matrices into blocks and working out the rules for the individual blocks. So in this post, we&#39;ll review the pushforward for square \(A\), also working out its pullback, and then we&#39;ll do the same for wide and tall \(A\).</p>
<h2 id="square_a">Square \(A\)</h2>
<p>A pushforward for the LU decomposition for square \(A\) is already known <sup id="fnref:HoogAnderssenLukas2011">[1]</sup>. For completeness, I&#39;ve included its derivation, as well as one for the corresponding pullback.</p>
<h3 id="pushforward">Pushforward</h3>
<p>We start by differentiating the defining equation:</p>
\[P \dot{A} = \dot{L} U + L \dot{U}\]
<p>We can solve both sides to get</p>
\[L^{-1} P \dot{A} U^{-1} = L^{-1} \dot{L} + \dot{U} U^{-1}\]
<p>\(\dot{L}\) and \(\dot{U}\) must be at least as sparse as \(L\) and \(U\), respectively. Hence, \(\dot{U}\) is upper triangular, and because the diagonal of \(L\) is constrained to be unit, \(\dot{L}\) will be lower triangular with a diagonal of zeros &#40;strictly lower triangular&#41;. Note also that the inverse of a lower/upper triangular matrix is still lower/upper triangular. Likewise, the product of two lower/upper triangular matrices is still lower/upper triangular.</p>
<p>Hence the right-hand side is the sum of a strictly lower triangular matrix \(L^{-1} \dot{L}\) and an upper triangular matrix \(\dot{U} U^{-1}\). Let&#39;s introduce the triangularizing operators \(\operatorname{triu}(X)\), which extracts the upper triangle of the matrix \(X\), and \(\operatorname{tril}_-(X)\), which extracts its strict lower triangle &#40;so that \(X = \operatorname{tril}_-(X) + \operatorname{triu}(X)\)&#41;.</p>
<p>Introducing an intermediate \(\dot{F}\), we can then solve for \(\dot{L}\) and \(\dot{U}\):</p>
\[\begin{aligned}
    \dot{F} &= L^{-1} P \dot{A} U^{-1}\\
    \dot{L} &= L \operatorname{tril}_-(\dot{F})\\
    \dot{U} &= \operatorname{triu}(\dot{F}) U
\end{aligned}\]
<h3 id="pullback">Pullback</h3>
<p>The corresponding pullback is</p>
\[\begin{aligned}
\overline{F} &= \operatorname{tril}_-(L^\mathrm{H} \overline{L}) + \operatorname{triu}(\overline{U} U^\mathrm{H})\\
\overline{A} &= P^\mathrm{T} L^{-\mathrm{H}} \overline{F} U^{-\mathrm{H}}
\end{aligned}\]
<p><details>      To find the pullback, we use the identity of reverse-mode differentiation and properties of the <a href="https://en.wikipedia.org/wiki/Frobenius_inner_product">Frobenius inner product</a> as described in <a href="https://www.juliadiff.org/ChainRulesCore.jl/stable/arrays.html">the ChainRules guide on deriving array rules</a>.</p>
<p>Here the identity takes the form</p>
\[\operatorname{Re}\left\langle  \overline{L},  \dot{L} \right\rangle + \operatorname{Re}\left\langle  \overline{U},  \dot{U} \right\rangle = \operatorname{Re}\left\langle  \overline{A},  \dot{A} \right\rangle.\]
<p>We want to solve for \(\overline{A}\), and we do so by first plugging \(\dot{U}\) and \(\dot{L}\) into the left-hand side of this identity, manipulating to look like the right-hand side, and then solving for \(\overline{A}\).</p>
\[\begin{aligned}
\left\langle  \overline{L},  \dot{L} \right\rangle &= \left\langle  \overline{L},  L \operatorname{tril}_-(\dot{F}) \right\rangle = \left\langle  L^\mathrm{H} \overline{L},  \operatorname{tril}_-(\dot{F}) \right\rangle\\
\left\langle  \overline{U},  \dot{U} \right\rangle &= \left\langle  \overline{U},  \operatorname{triu}(\dot{F}) U \right\rangle = \left\langle  \overline{U} U^\mathrm{H},  \operatorname{triu}(\dot{F}) \right\rangle\\
\end{aligned}\]
<p>Because the Frobenius inner product is the sum of all elements of the element-wise product of the second argument and the complex conjugate of the first argument, then for upper triangular \(U\), we have</p>
\[\left\langle  X,  U \right\rangle = \sum_{ij} X_{ij}^* U_{ij} = \sum_{ij} \operatorname{triu}(X)_{ij}^* U_{ij} = \left\langle  \operatorname{triu}(X),  U \right\rangle.\]
<p>The same is true for lower-triangular matrices &#40;or, analogously, for any sparsity pattern&#41;. Therefore, </p>
\[\begin{aligned}
\left\langle  \overline{L},  \dot{L} \right\rangle &= \left\langle  \operatorname{tril}_-(L^\mathrm{H} \overline{L}),  \dot{F} \right\rangle\\
\left\langle  \overline{U},  \dot{U} \right\rangle &= \left\langle  \operatorname{triu}(\overline{U} U^\mathrm{H}),  \dot{F} \right\rangle\\
\left\langle  \overline{L},  \dot{L} \right\rangle + \left\langle  \overline{U},  \dot{U} \right\rangle &= \left\langle  \operatorname{tril}_-(L^\mathrm{H} \overline{L}) + \operatorname{triu}(\overline{U} U^\mathrm{H}),  \dot{F} \right\rangle \doteq \left\langle  \overline{F},  \dot{F} \right\rangle,
\end{aligned}\]
<p>where we have introduced an intermediate \(\overline{F}\).</p>
<p>Continuing by plugging in \(\dot{F}\), we find</p>
\[\left\langle  \overline{F},  \dot{F} \right\rangle = \left\langle  \overline{F},  L^{-1} P \dot{A} U^{-1} \right\rangle = \left\langle  P^\mathrm{T} L^{-\mathrm{H}} \overline{F} U^{-\mathrm{H}},  \dot{A} \right\rangle\]
<p>So the pullback of the LU decomposition is written</p>
\[\begin{aligned}
\overline{F} &= \operatorname{tril}_-(L^\mathrm{H} \overline{L}) + \operatorname{triu}(\overline{U} U^\mathrm{H})\\
\overline{A} &= P^\mathrm{T} L^{-\mathrm{H}} \overline{F} U^{-\mathrm{H}}
\end{aligned}\]
<p></details><br/></p>
<p>Note that these expressions use the same elementary operations as solving a system of equations using the LU decomposition, as noted above. The pushforwards and pullbacks can then be computed in-place with no additional allocations.</p>
<h2 id="wide_a">Wide \(A\)</h2>
<p>We can write wide \(A\) in blocks \(A = \begin{bmatrix}A_1 & A_2 \end{bmatrix}\), where \(A_1 \in \mathbb{C}^{m \times m}\) and \(A_2  \in \mathbb{C}^{m \times (n - m)}\). It will turn out to be very convenient that \(A_1\) is square. The LU decomposition in terms of these blocks is written</p>
\[P\begin{bmatrix}A_1 & A_2\end{bmatrix} = L \begin{bmatrix}U_1 & U_2\end{bmatrix},\]
<p>where \(U_1\) is square upper triangular. This is a system of two equations that we will address separately.</p>
\[\begin{aligned}
P A_1 &= L U_1\\
P A_2 &= L U_2\\
\end{aligned}\]
<h3 id="pushforward__2">Pushforward</h3>
<p>Introducing an intermediate \(\dot{H} = \begin{bmatrix} \dot{H}_1 & \dot{H}_2 \end{bmatrix}\) with the same block structure as \(U\), the complete pushforward is</p>
\[\begin{aligned}
    \dot{H} &= L^{-1} P \dot{A}\\
    \dot{F} &= \dot{H}_1 U_1^{-1}\\
    \dot{U}_1 &= \operatorname{triu}(\dot{F}) U_1\\
    \dot{U}_2 &= \dot{H}_2 - \operatorname{tril}_-(\dot{F}) U_2\\
    \dot{L} &= L \operatorname{tril}_-(\dot{F})\\
\end{aligned}\]
<p><details>      Note that the first equation is identical in form to the square LU decomposition. So we can reuse that solution for the pushforward to get</p>
\[\begin{aligned}
    \dot{F} &= L^{-1} P \dot{A}_1 U_1^{-1}\\
    \dot{L} &= L \operatorname{tril}_-(\dot{F})\\
    \dot{U}_1 &= \operatorname{triu}(\dot{F}) U_1
\end{aligned}\]
<p>Now, let&#39;s differentiate the second equation</p>
\[P \dot{A}_2 = \dot{L} U_2 + L \dot{U}_2\]
<p>and solve for \(\dot{U}_2\)</p>
\[\dot{U}_2 = L^{-1} P \dot{A}_2 - L^{-1} \dot{L} U_2.\]
<p>Plugging in our previous solution for \(\dot{L}\), we find</p>
\[\dot{U}_2 = L^{-1} P \dot{A}_2 - \operatorname{tril}_-(\dot{F}) U_2.\]
<p></details><br/></p>
<h3 id="pullback__2">Pullback</h3>
<p>Introducing an intermediate \(\overline{H} = \begin{bmatrix} \overline{H}_1 & \overline{H}_2 \end{bmatrix}\) with the same block structure as \(U\), the corresponding pullback is</p>
\[\begin{aligned}
\overline{H}_1 &= \left(\operatorname{tril}_-(L^\mathrm{H} \overline{L} - \overline{U}_2 U_2^\mathrm{H}) + \operatorname{triu}(\overline{U}_1 U_1^\mathrm{H})\right) U_1^{-\mathrm{H}}\\
\overline{H}_2 &= \overline{U}_2\\
\overline{A} &= P^\mathrm{T} L^{-\mathrm{H}} \overline{H}
\end{aligned}\]
<p><details>      Here the reverse-mode identity is</p>
\[\operatorname{Re}\left\langle  \overline{L},  \dot{L} \right\rangle + \operatorname{Re}\left\langle  \overline{U}_1,  \dot{U}_1 \right\rangle + \operatorname{Re}\left\langle  \overline{U}_2,  \dot{U}_2 \right\rangle = \operatorname{Re}\left\langle  \overline{A},  \dot{A} \right\rangle.\]
<p>We plug in \(\dot{L}\), \(\dot{U}_1\), and \(\dot{U}_2\) to find</p>
\[\begin{aligned}
& \operatorname{Re}\left\langle  \overline{L},  L \operatorname{tril}_-(\dot{F}) \right\rangle + \operatorname{Re}\left\langle  \overline{U}_1,  \operatorname{triu}(\dot{F}) U_1 \right\rangle + \operatorname{Re}\left\langle  \overline{U}_2,  \dot{H}_2 - \operatorname{tril}_-(\dot{F}) U_2 \right\rangle\\
&= \operatorname{Re}\left\langle  \operatorname{tril}_-(L^\mathrm{H} \overline{L}) - \operatorname{tril}_-(\overline{U}_2 U_2^\mathrm{H}) + \operatorname{triu}(\overline{U}_1 U_1^\mathrm{H}),  \dot{F} \right\rangle + \operatorname{Re}\left\langle  \overline{U}_2,  \dot{H}_2 \right\rangle\\
&= \operatorname{Re}\left\langle  \left(\operatorname{tril}_-(L^\mathrm{H} \overline{L}) - \operatorname{tril}_-(\overline{U}_2 U_2^\mathrm{H}) + \operatorname{triu}(\overline{U}_1 U_1^\mathrm{H})\right) U_1^{-\mathrm{H}},  \dot{H}_1 \right\rangle + \operatorname{Re}\left\langle  \overline{U}_2,  \dot{H}_2 \right\rangle
\end{aligned}\]
<p>Let&#39;s introduce the intermediates</p>
\[\begin{aligned}
\overline{H}_1 &= \left(\operatorname{tril}_-(L^\mathrm{H} \overline{L} - \overline{U}_2 U_2^\mathrm{H}) + \operatorname{triu}(\overline{U}_1 U_1^\mathrm{H})\right) U_1^{-\mathrm{H}}\\
\overline{H}_2 &= \overline{U}_2,
\end{aligned}\]
<p>which like \(\dot{H}\) we organize into the block matrix \(\overline{H} = \begin{bmatrix} \overline{H}_1 & \overline{H}_2 \end{bmatrix}\). This block structure lets us rewrite the above identity in terms of \(\overline{H}\) and \(\dot{H}\):</p>
\[\operatorname{Re}\left\langle  \overline{H}_1,  \dot{H}_1 \right\rangle + \operatorname{Re}\left\langle  \overline{H}_2,  \dot{H}_2 \right\rangle = \operatorname{Re}\left\langle  \overline{H},  \dot{H} \right\rangle\]
<p>Now we plug in \(\dot{H}\)</p>
\[\operatorname{Re}\left\langle  \overline{H},  \dot{H} \right\rangle = \operatorname{Re}\left\langle  \overline{H},  L^{-1} P \dot{A} \right\rangle = \operatorname{Re}\left\langle  P^\mathrm{T} L^{-\mathrm{H}} \overline{H},  \dot{A} \right\rangle\]
<p>We have arrived at the desired form and can solve for \(\overline{A}\):</p>
\[\overline{A} = P^\mathrm{T} L^{-\mathrm{H}} \overline{H}.\]
<p></details><br/></p>
<h2 id="tall_a">Tall \(A\)</h2>
<p>The tall case is very similar to the wide case, except now we have the block structure</p>
\[\begin{bmatrix}P_1 \\ P_2 \end{bmatrix} A = \begin{bmatrix}L_1 \\ L_2 \end{bmatrix} U,\]
<p>where now \(L_1\) is square unit lower triangular, and \(U\) is square upper triangular. This gives us the system of equations</p>
\[\begin{aligned}
    P_1 A &= L_1 U\\
    P_2 A &= L_2 U.
\end{aligned}\]
<h3 id="pushforward__3">Pushforward</h3>
<p>The first equation is again identical to the square case, so we can use it to solve for \(\dot{L}_1\) and \(\dot{U}\). Likewise, the same approach we used to solve \(\dot{U}_2\) in the wide case can be applied here to solve for \(\dot{L}_2\).</p>
<p>Introducing an intermediate \(\dot{H} = \begin{bmatrix} \dot{H}_1 \\ \dot{H}_2 \end{bmatrix}\) with the same block structure as \(L\), the complete pushforward is</p>
\[\begin{aligned}
    \dot{H} &= P \dot{A} U^{-1}\\
    \dot{F} &= L_1^{-1} \dot{H}_1\\
    \dot{L}_1 &= L_1 \operatorname{tril}_-(\dot{F})\\
    \dot{L}_2 &= \dot{H}_2 - L_2 \operatorname{triu}(\dot{F})\\
    \dot{U} &= \operatorname{triu}(\dot{F}) U
\end{aligned}\]
<h3 id="pullback__3">Pullback</h3>
<p>Introducing an intermediate \(\overline{H} = \begin{bmatrix} \overline{H}_1 \\ \overline{H}_2 \end{bmatrix}\) with the same block structure as \(L\), the corresponding pullback is</p>
\[\begin{aligned}
\overline{H}_1 &= L_1^{-\mathrm{H}} \left(\operatorname{tril}_-(L_1^\mathrm{H} \overline{L}_1) +  \operatorname{triu}(\overline{U} U^\mathrm{H} - L_2^\mathrm{H} \overline{L}_2)\right)\\
\overline{H}_2 &= \overline{L}_2\\
\overline{A} &= P^\mathrm{T} \overline{H} U^{-\mathrm{H}}
\end{aligned}\]
<h2 id="implementation">Implementation</h2>
<p>The product of this derivation is <a href="https://github.com/JuliaDiff/ChainRules.jl/pull/354">this pull request to ChainRules.jl</a>, which includes tests of the rules using <a href="https://github.com/JuliaDiff/FiniteDifferences.jl">FiniteDifferences.jl</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The techniques employed here are general and can be used for differentiation rules for other factorizations of non-square matrices. A recent paper used a similar approach to derive the pushforwards and pullbacks of the \(QR\) and \(LQ\) decompositions<sup id="fnref:RobertsRoberts2020">[2]</sup>.</p>
<h2 id="references">References</h2>
<p><table class="fndef" id="fndef:HoogAnderssenLukas2011">
    <tr>
        <td class="fndef-backref">[1]</td>
        <td class="fndef-content">de Hoog F.R., Anderssen R.S., and Lukas M.A. &#40;2011&#41; Differentiation of matrix functionals using triangular factorization. Mathematics of Computation, 80 &#40;275&#41;. p. 1585. doi: <a href="http://doi.org/10.1090/S0025-5718-2011-02451-8">10.1090/S0025-5718-2011-02451-8</a>.</td>
    </tr>
</table>
<table class="fndef" id="fndef:RobertsRoberts2020">
    <tr>
        <td class="fndef-backref">[2]</td>
        <td class="fndef-content">Roberts D.A.O. and Roberts L.R. &#40;2020&#41; QR and LQ Decomposition Matrix Backpropagation Algorithms for Square, Wide, and Deep – Real or Complex – Matrices and Their Software Implementation. arXiv: <a href="https://arxiv.org/abs/2009.10071">2009.10071</a>.</td>
    </tr>
</table>
</p>
 ]]>
  </content:encoded>
    
  <pubDate>Wed, 03 Feb 2021 00:00:00 +0000</pubDate>  
  
  
  <atom:author>
    <atom:name>Seth Axen</atom:name>
  </atom:author>
        
</item>
</channel></rss>